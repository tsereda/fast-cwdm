apiVersion: v1
kind: Pod
metadata:
  name: infer-fast-cwdm
  labels:
    app: fast-cwdm
spec:
  nodeSelector:
    nautilus.io/linstor: "true"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: nvidia.com/gpu.product
            operator: In
            values:
            - NVIDIA-A10
            - NVIDIA-GeForce-RTX-3090
            - NVIDIA-L40
            - NVIDIA-A40

  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
    - key: nvidia.com/gpu
      operator: Exists
      effect: PreferNoSchedule
  containers:
    - name: brats-processing
      image: gitlab-registry.nrp-nautilus.io/prp/jupyter-stack/prp
      env:
        - name: REPO_PATH
          value: /app/fast-cwdm
        - name: PYTHONPATH
          value: /app/fast-cwdm
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: PYTHONUNBUFFERED
          value: "1"
        - name: PYTHONIOENCODING
          value: "UTF-8"
        - name: WANDB_MODE
          value: "offline"  # Disable wandb for inference
      command: ["/bin/bash", "-c"]
      args:
        - |
          git clone https://github.com/tsereda/fast-cwdm.git ${REPO_PATH}
          cd ${REPO_PATH}
          
          sudo apt-get update && sudo apt-get install -y p7zip-full wget git
          
          # Extract datasets
          for dataset in "TrainingData" "ValidationData"; do
            zip_file="/data/ASNR-MICCAI-BraTS2023-GLI-Challenge-${dataset}.zip"
            if [ -f "$zip_file" ]; then
              echo "Extracting ${dataset}..."
              cd /data && 7z x "$zip_file" -y
              cd ${REPO_PATH}
            fi
          done

          # Create directory structure
          mkdir -p datasets/BRATS2023/training
          mkdir -p datasets/BRATS2023/validation
          
          if [ -d "/data/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData" ]; then
            echo "Copying training data..."
            cp -r /data/ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData/* datasets/BRATS2023/training/
            echo "Training data copied successfully"
          fi
          
          if [ -d "/data/ASNR-MICCAI-BraTS2023-GLI-Challenge-ValidationData" ]; then
            echo "Copying validation data..."
            cp -r /data/ASNR-MICCAI-BraTS2023-GLI-Challenge-ValidationData/* datasets/BRATS2023/validation/
            echo "Validation data copied successfully"
          fi

          # Install dependencies
          pip install nibabel blobfile wandb batchgenerators simpleITK gdown
          
          echo "Creating pseudo validation set..."
          python scripts/drop_modality.py
          
          # Create checkpoints directory and copy models
          mkdir -p ./checkpoints
          
          echo "Available checkpoints in /data/checkpoints:"
          ls -la /data/checkpoints/*.pt
          
          # Copy the BEST models and rename them to expected format
          if [ -f "/data/checkpoints/brats_t1n_BEST_direct_1000.pt" ]; then
            cp /data/checkpoints/brats_t1n_BEST_direct_1000.pt ./checkpoints/brats_t1n_BEST_direct_1000.pt
            echo "Copied t1n model"
          fi
          
          if [ -f "/data/checkpoints/brats_t1c_BEST_direct_1000.pt" ]; then
            cp /data/checkpoints/brats_t1c_BEST_direct_1000.pt ./checkpoints/brats_t1c_BEST_direct_1000.pt
            echo "Copied t1c model"
          fi
          
          if [ -f "/data/checkpoints/brats_t2w_BEST_direct_1000.pt" ]; then
            cp /data/checkpoints/brats_t2w_BEST_direct_1000.pt ./checkpoints/brats_t2w_BEST_direct_1000.pt
            echo "Copied t2w model"
          fi
          
          if [ -f "/data/checkpoints/brats_t2f_BEST_direct_1000.pt" ]; then
            cp /data/checkpoints/brats_t2f_BEST_direct_1000.pt ./checkpoints/brats_t2f_BEST_direct_1000.pt
            echo "Copied t2f model"
          fi
          
          echo "Models copied:" 
          ls -la ./checkpoints/
          
          echo "Running inference..."
          python scripts/complete_dataset.py \
            --input_dir ./datasets/BRATS2023/pseudo_validation \
            --output_dir ./datasets/BRATS2023/pseudo_validation_completed \
            --checkpoint_dir ./checkpoints \
            --device cuda:0 \
            --max_cases 5

          echo "Inference completed! Results:"
          ls -la ./datasets/BRATS2023/pseudo_validation_completed/

          tail -f /dev/null
     
      volumeMounts:
        - name: workspace
          mountPath: /app
        - name: data
          mountPath: /data
        - name: shm
          mountPath: /dev/shm
    
      resources:
        requests:
          memory: 24Gi
          cpu: "12"
          nvidia.com/gpu: "1"
        limits:
          memory: 32Gi
          cpu: "16"
          nvidia.com/gpu: "1"
 
  volumes:
    - name: workspace
      emptyDir:
        sizeLimit: 50Gi
    - name: data
      persistentVolumeClaim:
        claimName: brats2025-3
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 16Gi
 
  restartPolicy: Never